{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Things\n",
    "\n",
    "from data.volumes import Volume, Page, Line\n",
    "from data.graphset import Graphset\n",
    "from data.graph_sampler import GraphSampler, AttributeSampler\n",
    "from data.image_dataset import ImageDataset\n",
    "\n",
    "\n",
    "import torch_geometric.utils as tutils\n",
    "\n",
    "\n",
    "import data.volumes as dv\n",
    "\n",
    "## Model Things\n",
    "from models import gnn_encoders as gnn\n",
    "from models import visual_encoders as cnn\n",
    "from models import edge_visual_encoders as EVE\n",
    "from models.graph_construction_model import MMGCM\n",
    "\n",
    "\n",
    "### Utils\n",
    "import utils \n",
    "import visualizations as visu\n",
    "\n",
    "\n",
    "## Pipelines\n",
    "import pipelines as pipes\n",
    "\n",
    "## tasks\n",
    "from tasks import record_linkage as rl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import requests\n",
    "import cv2\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from typing import *\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydra Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides = [\n",
    "    \"data.dataset.path=../data/CED/SFLL\",\n",
    "    \"models.edge_visual_encoder.add_attention=False\",\n",
    "    \"models.add_language=False\",    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=\"1.3.2\", config_path=\"./configs\"):\n",
    "    CFG = compose(config_name=\"eval\", overrides=overrides, return_hydra_config=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_models = CFG.models\n",
    "cfg_data = CFG.data\n",
    "cfg_setup = CFG.setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! The utility of this dictionary is to relate the groundtruth with the visual information AD-HOC\n",
    "pk = {\"Noms_harmo\":\"nom\", \"cognom1_harmo\":\"cognom_1\", \"cognom2_harmo\":\"cognom_2\", \"parentesc_har\":\"parentesc\", \"ocupacio\":\"ocupacio\"}\n",
    "\n",
    "#  ^ Hydra things\n",
    "\n",
    "batch_size = cfg_data.collator.batch_size\n",
    "shuffle = cfg_data.collator.shuffle\n",
    "number_volum_years = len(cfg_data.dataset.volumes) \n",
    "checkpoint_name = cfg_models.name_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MMGC_Experiment_1_New_Edge_PE_no_Attention_language'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_models.edge_visual_encoder.add_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING DOWNLOADING VOLUMES: VOLUME- ../data/CED/SFLL/1889\n",
      "050000120052048,0014.jpg\n",
      "050000120052048,0031.jpg\n",
      "050000120052048,0060.jpg\n",
      "050000120052048,0061.jpg\n",
      "050000120052048,0074.jpg\n",
      "050000120052048,0082.jpg\n",
      "050000120052048,0088.jpg\n",
      "050000120052048,0091.jpg\n",
      "STARTING DOWNLOADING VOLUMES: VOLUME- ../data/CED/SFLL/1906\n",
      "050000120052053,0007.jpg\n",
      "050000120052053,0022.jpg\n",
      "050000120052053,0058.jpg\n",
      "050000120052053,0059.jpg\n",
      "050000120052053,0111.jpg\n",
      "STARTING DOWNLOADING VOLUMES: VOLUME- ../data/CED/SFLL/1910\n",
      "050000120052054,0007.jpg\n",
      "050000120052054,0021.jpg\n",
      "050000120052054,0032.jpg\n",
      "050000120052054,0035.jpg\n",
      "050000120052054,0040.jpg\n",
      "050000120052054,0055.jpg\n",
      "050000120052054,0068.jpg\n",
      "050000120052054,0073.jpg\n",
      "STARTING DOWNLOADING VOLUMES: VOLUME- ../data/CED/SFLL/1915\n",
      "050000120052055,0007.jpg\n",
      "050000120052055,0008.jpg\n",
      "050000120052055,0012.jpg\n",
      "050000120052055,0013.jpg\n",
      "050000120052055,0015.jpg\n",
      "050000120052055,0031.jpg\n",
      "050000120052055,0037.jpg\n",
      "050000120052055,0048.jpg\n",
      "050000120052055,0074.jpg\n",
      "050000120052055,0107.jpg\n",
      "050000120052055,0110.jpg\n",
      "050000120052055,0129.jpg\n",
      "Image Lines Recovered with no permutation inconsitences:  13829\n",
      "Total Lines :  14860\n",
      "Total proportion :  0.9306191205978394 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Adj for attributes: 462it [00:00, 816.42it/s] \n",
      "Generating Adj for attributes: 1086it [00:02, 383.86it/s]\n",
      "Generating Adj for attributes: 1338it [00:00, 4723.54it/s]\n",
      "Generating Adj for Same As: 5486it [00:00, 41770.58it/s]\n",
      "Generating Adj for families: 3045it [00:00, 19591.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DataLoader\n",
      "DATA LOADED SUCCESFULLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## & Extract the dataset and the information in this case \n",
    "volumes = pipes.load_volumes(cfg=cfg_data) \n",
    "image_dataset = ImageDataset(Volumes=volumes, cfg=cfg_data.dataset)\n",
    "df_transcriptions = image_dataset._total_gt\n",
    "n_different_individuals = image_dataset._total_individual_nodes\n",
    "graphset = Graphset(total_nodes=n_different_individuals,\n",
    "                    df_transcriptions=df_transcriptions,\n",
    "                    n_volumes=len(volumes),\n",
    "                    graph_configuration=cfg_data.graph_configuration,\n",
    "                    auxiliar_entities_pk = pk)\n",
    "\n",
    "sampler = AttributeSampler(graph=graphset._graph, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "print(\"Generating DataLoader\")\n",
    "\n",
    "total_loader = DataLoader(dataset=image_dataset, \n",
    "                        batch_size = batch_size,\n",
    "                        collate_fn=image_dataset.collate_fn,\n",
    "                        num_workers=0,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True)\n",
    "\n",
    "print(\"DATA LOADED SUCCESFULLY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL LOADED SUCCESFULLY\n"
     ]
    }
   ],
   "source": [
    "## Model Things\n",
    "from models import gnn_encoders as gnn\n",
    "from models import visual_encoders as cnn\n",
    "from models import edge_visual_encoders as EVE\n",
    "from models.graph_construction_model import MMGCM\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "\n",
    "H = image_dataset.general_height \n",
    "W = image_dataset.general_width // 16\n",
    "\n",
    "cfg_models.edge_visual_encoder.input_atention_mechanism = H*W\n",
    "\n",
    "model = MMGCM(visual_encoder=cnn.LineFeatureExtractor, gnn_encoder=gnn.AttributeGNN, edge_encoder=EVE.EdgeAttFeatureExtractor, cfg=cfg_models).to(device)\n",
    "model_name = f\"../checkpoints/{checkpoint_name}.pt\"\n",
    "name_embeddings = f\"{checkpoint_name}\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.to(device)\n",
    "print(\"MODEL LOADED SUCCESFULLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTING LANGUAGE EMBEDDINGS\n",
      "EXTRACTING VISUAL EMBEDDINGS\n",
      "Embeddings Loaded Succesfully\n"
     ]
    }
   ],
   "source": [
    "print(\"EXTRACTING LANGUAGE EMBEDDINGS\")\n",
    "filepath  = f\"../embeddings/language_embeddings_{number_volum_years}_entities_{len(cfg_setup.configuration.compute_loss_on)}.pkl\"\n",
    "language_embeddings = utils.read_pickle(filepath)\n",
    "\n",
    "print(\"EXTRACTING VISUAL EMBEDDINGS\")\n",
    "attribute_embeddings = utils.read_pickle(filepath=f\"../embeddings/{checkpoint_name}.pkl\")\n",
    "attribute_embeddings = attribute_embeddings.cpu()\n",
    "\n",
    "\n",
    "print(\"Embeddings Loaded Succesfully\")\n",
    "\n",
    "graph = graphset._graph\n",
    "graph.x_language = language_embeddings\n",
    "graph.epoch_populations = image_dataset._population_per_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting distributions and analysi of the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_save_plot = f\"../plots/evaluation/{checkpoint_name}/\"\n",
    "base_save_metrics = f\"../metrics/evaluation/{checkpoint_name}/\"\n",
    "os.makedirs(base_save_plot, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the most common transcriptions and the least common transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_name_attributes = {label: indexes for label, indexes in graph[\"nom\"].map_attribute_index.items() if len(indexes) > 20}\n",
    "filtered_surname_attributes = {label: indexes for label, indexes in graph[\"cognom_1\"].map_attribute_index.items() if len(indexes) > 20}\n",
    "filtered_ssurname_attributes = {label: indexes for label, indexes in graph[\"cognom_2\"].map_attribute_index.items() if len(indexes) > 20} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a random name, surname and second surname\n",
    "random_name = random.sample((filtered_name_attributes.keys()), 1)[0]\n",
    "random_surname = random.sample((filtered_surname_attributes.keys()), 1)[0]\n",
    "random_second_surname = random.sample((filtered_ssurname_attributes.keys()), 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bonaventura'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the drift of the whole population before and after the model feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13829, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "print(attribute_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = graph[\"nom\"].map_attribute_index[random_name]\n",
    "nom_idx = list(graph.map_attribute_nodes.values()).index(\"nom\")\n",
    "nom_embeddings = attribute_embeddings[:, nom_idx, :].numpy()\n",
    "previous_embeddings = copy.copy(graph.x_attributes).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   59,    73,  1481,  1483,  1715,  1902,  2272,  2379,  2484,\n",
       "        3860,  3935,  4598,  4709,  6186,  6448,  6499,  6532,  6681,\n",
       "        6909,  7392,  7467,  8837,  9363,  9920, 10322, 10534, 11585,\n",
       "       12479, 12522, 12622, 13286])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu.plot_shift(embedding_matrix_1=previous_embeddings[population, nom_idx,:],\n",
    "                embedding_matrix_2= nom_embeddings[population],\n",
    "                fig_path=os.path.join(base_save_plot, \"nom\", f\"shift_{random_name}.jpg\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu.plot_embedding_distribution(nom_embeddings, os.path.join(base_save_plot, \"nom\", f\"distribution.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = graph[\"cognom_1\"].map_attribute_index[random_surname]\n",
    "cognom_idx = list(graph.map_attribute_nodes.values()).index(\"cognom_1\")\n",
    "cognom_embeddings = attribute_embeddings[:, cognom_idx, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu.plot_shift(embedding_matrix_1=previous_embeddings[population, cognom_idx,:],\n",
    "                embedding_matrix_2= cognom_embeddings[population],\n",
    "                fig_path=os.path.join(base_save_plot, \"cognom_1\", f\"shift_{random_surname}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu.plot_embedding_distribution(cognom_embeddings, os.path.join(base_save_plot, \"cognom_1\", f\"distribution.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = graph[\"cognom_2\"].map_attribute_index[random_second_surname]\n",
    "cognom2_idx = list(graph.map_attribute_nodes.values()).index(\"cognom_2\")\n",
    "cognom2_embeddings = attribute_embeddings[:, cognom2_idx, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu.plot_shift(embedding_matrix_1=previous_embeddings[population, cognom2_idx,:],\n",
    "                embedding_matrix_2 = cognom2_embeddings[population],\n",
    "                fig_path=os.path.join(base_save_plot, \"cognom_2\", f\"shift_{random_second_surname}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu.plot_embedding_distribution(cognom2_embeddings, os.path.join(base_save_plot, \"cognom_2\", f\"distribution.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the distance metri space from each of the attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cluster distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_freq_name = {name: len(values) for name, values in graph[\"nom\"].map_attribute_index.items() if len(values) > 10}\n",
    "computed_freq_surname = {name: len(values) for name, values in graph[\"cognom_1\"].map_attribute_index.items() if len(values) > 10}\n",
    "computed_freq_ssurname = {name: len(values) for name, values in graph[\"cognom_2\"].map_attribute_index.items() if len(values) > 10}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_freq  = sorted(list(computed_freq_name.items()), key= lambda k: (k[1]), reverse=True)\n",
    "surname_freq  = sorted(list(computed_freq_surname.items()), key= lambda k: (k[1]), reverse=True)\n",
    "second_surnname_freq  = sorted(list(computed_freq_ssurname.items()), key= lambda k: (k[1]), reverse=True)\n",
    "\n",
    "most_common_names, less_common_names = name_freq[:10], name_freq[-10:]\n",
    "most_common_surnames, less_common_surnames = surname_freq[:10], surname_freq[-10:]\n",
    "most_common_ssurnames, less_common_ssurnames = second_surnname_freq[:10], second_surnname_freq[-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('josep', 1114),\n",
       " ('maria', 999),\n",
       " ('joan', 845),\n",
       " ('josefa', 536),\n",
       " ('jaume', 491),\n",
       " ('dolores', 471),\n",
       " ('teresa', 401),\n",
       " ('rosa', 393),\n",
       " ('francisco', 391),\n",
       " ('antonio', 378)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('avelina', 11),\n",
       " ('clotilde', 11),\n",
       " ('eusebi', 11),\n",
       " ('llucia', 11),\n",
       " ('pasqual', 11),\n",
       " ('rafela', 11),\n",
       " ('remigia', 11),\n",
       " ('serafina', 11),\n",
       " ('sever', 11),\n",
       " ('ursicina', 11)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_common_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepp_indexes_name = [(label, graph[\"nom\"].map_attribute_index[label]) for label, _ in most_common_names + less_common_names  ]\n",
    "visu.plot_violin_plot_from_freq_attribute_distances(specific_attribute_embeddings=attribute_embeddings[:,0,:], dic_realtion_names=kepp_indexes_name,\n",
    "                                                    file_path=os.path.join(base_save_plot, \"nom\", f\"common_distances_distribution.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepp_indexes_surname = [(label, graph[\"cognom_1\"].map_attribute_index[label]) for label, _ in most_common_surnames + less_common_surnames  ]\n",
    "visu.plot_violin_plot_from_freq_attribute_distances(specific_attribute_embeddings=attribute_embeddings[:,1,:], dic_realtion_names=kepp_indexes_surname,\n",
    "                                                    file_path=os.path.join(base_save_plot, \"cognom_1\", f\"common_distances_distribution.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepp_indexes_ssurname = [(label, graph[\"cognom_2\"].map_attribute_index[label]) for label, _ in most_common_ssurnames + less_common_ssurnames  ]\n",
    "visu.plot_violin_plot_from_freq_attribute_distances(specific_attribute_embeddings=attribute_embeddings[:,2,:], dic_realtion_names=kepp_indexes_ssurname,\n",
    "                                                    file_path=os.path.join(base_save_plot, \"cognom_2\", f\"common_distances_distribution.jpg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu.plot_attribute_metric_space(attribute_embedding_space=attribute_embeddings.cpu(),\n",
    "                                    fig_name=os.path.join(base_save_plot, \"Attributes_Distribution.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the nearest Neighboors of the different attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_precision(relevant_documents, retrieved_documents):\n",
    "    \"\"\"\n",
    "    Calculate the R-Precision metric.\n",
    "    \n",
    "    Parameters:\n",
    "    relevant_documents (list): List of relevant document IDs.\n",
    "    retrieved_documents (list): List of retrieved document IDs in ranked order.\n",
    "\n",
    "    Returns:\n",
    "    float: R-Precision score.\n",
    "    \"\"\"\n",
    "    # The number of relevant documents in the ground truth\n",
    "    R = len(relevant_documents)\n",
    "    \n",
    "    # Take the top-R retrieved documents\n",
    "    top_r_retrieved = retrieved_documents[:R]\n",
    "    \n",
    "    # Count how many of the top-R retrieved documents are relevant\n",
    "    relevant_retrieved_count = len(set(top_r_retrieved) & set(relevant_documents))\n",
    "    \n",
    "    # R-Precision is the ratio of relevant documents retrieved in top-R to R\n",
    "    return relevant_retrieved_count / (R) \n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_intra_cluster_nearest_neighboors_at_k(x,\n",
    "                                                  top_k=10):\n",
    "    #Graph = utils.read_pickle(\"pickles/graphset_3_volumes_128_entities_4.pkl\")\n",
    "\n",
    "    dict_nearest_neighbors = {}\n",
    "    distances_dict = {}\n",
    "    for idx in range(x.shape[1]):\n",
    "        embeddings = x[:, idx]\n",
    "        distances = torch.cdist(embeddings, embeddings, p=2)\n",
    "        distances_dict[idx] = distances\n",
    "        \n",
    "        # Set the diagonal to a large positive number to avoid self-matching\n",
    "        distances.fill_diagonal_(float('inf'))\n",
    "        # Get the top K nearest neighbors for each embedding (smallest distances)\n",
    "        top_k_values, top_k_indices = torch.topk(distances, top_k, dim=1, largest=False)\n",
    "\n",
    "        # Create a dictionary mapping each index to its top K nearest neighbors\n",
    "        nearest_neighbors = {ix: top_k_indices[ix].tolist() for ix in range(distances.size(0))}\n",
    "\n",
    "        dict_nearest_neighbors[idx] = nearest_neighbors\n",
    "        \n",
    "    return dict_nearest_neighbors, distances_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric space of the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2948, 1, 128])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_embeddings = torch.from_numpy(np.array(language_embeddings)).unsqueeze(1)\n",
    "language_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_lines = image_dataset._line_paths\n",
    "nn, distances = extract_intra_cluster_nearest_neighboors_at_k(language_embeddings, top_k=language_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ocrs = {\"nom\": [], \"cognom_1\":[], \"cognom_2\": []}\n",
    "for nom, cog, cog2 in (image_dataset._ocrs):\n",
    "    \n",
    "    list_ocrs[\"nom\"].append(image_dataset._map_ocr[nom])\n",
    "    list_ocrs[\"cognom_1\"].append(image_dataset._map_ocr[cog])\n",
    "    list_ocrs[\"cognom_2\"].append(image_dataset._map_ocr[cog2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[409, 870, 1181, 1216, 1739, 165, 1580, 942, 2417, 1626, 1340, 1047, 2892, 1062, 856, 2073, 1125, 2263, 1817, 1599, 2752, 1801, 1271, 2745, 40, 150, 2035, 1404, 61, 387, 151, 2591, 2777, 607, 1428, 358, 2022, 82, 1896, 1057, 784, 2880, 2307, 2068, 2415, 940, 2424, 1035, 2798, 1990, 212, 2775, 1128, 2152, 168, 808, 1302, 627, 1827, 1657, 1263, 648, 293, 139, 2945, 1796, 1018, 2504, 560, 689, 441, 1256, 1681, 598, 1655, 75, 1036, 558, 1516, 864, 2580, 544, 2054, 1251, 2429, 2122, 1594, 1810, 1237, 952, 1856, 481, 490, 2261, 1220, 561, 226, 2917, 1994, 731, 990, 1520, 919, 1792, 805, 2361, 1793, 2649, 1565, 2146, 1679, 1704, 2653, 236, 2379, 2252, 2327, 1038, 1577, 1713, 987, 112, 1134, 2593, 2803, 2249, 674, 1821, 1940, 1208, 158, 2265, 721, 228, 2298, 1582, 1384, 1575, 2763, 2804, 1299, 2853, 2200, 2847, 1678, 2300, 834, 1978, 2270, 1332, 688, 2195, 2784, 2872, 1068, 2145, 2622, 2808, 1190, 1020, 1794, 1506, 2127, 2744, 923, 16, 1026, 2102, 2633, 1938, 1021, 2174, 532, 2720, 1439, 2490, 2250, 820, 98, 1408, 107, 2117, 1444, 365, 224, 1891, 1715, 664, 2581, 878, 2320, 2286, 2225, 773, 2466, 322, 945, 1379, 2161, 1135, 181, 2293, 159, 996, 1904, 1083, 2585, 1370, 670, 924, 308, 2227, 711, 2636, 2879, 2129, 1226, 646, 1942, 884, 1397, 2071, 1992, 949, 218, 1438, 1727, 2566, 1751, 1882, 1476, 1625, 2656, 2007, 1495, 1445, 767, 1046, 2459, 1860, 1621, 1522, 2470, 2061, 1741, 2811, 2799, 1147, 305, 2202, 2722, 2641, 1174, 1545, 2818, 647, 2020, 1939, 847, 1509, 2473, 2499, 1425, 2493, 2669, 2883, 146, 725, 723, 2376, 1110, 2885, 1105, 388, 636, 2057, 2899, 1141, 2234, 2578, 1268, 1629, 1602, 709, 2308, 1769, 1502, 1646, 1165, 676, 807, 103, 814, 1746, 658, 2439, 1318, 1868, 935, 1145, 1969, 384, 750, 1525, 759, 1177, 232, 79, 2310, 1434, 55, 2508, 489, 2328, 1293, 2751, 1871, 584, 741, 2031, 1596, 2201, 1156, 1358, 2165, 946, 1382, 48, 2602, 1488, 2739, 2694, 354, 230, 728, 1169, 826, 1571, 1157, 720, 231, 944, 2795, 760, 1233, 1537, 1255, 1584, 2678, 1443, 157, 462, 1683, 2329, 2345, 2941, 2273, 783, 2935, 296, 900, 1224, 2867, 542, 2502, 2788, 2452, 1578, 1059, 546, 1242, 1356, 954, 2430, 1151, 654, 2778, 2411, 1343, 1478, 2326, 562, 1987, 199, 1365, 1215, 960, 1682, 317, 102, 515, 1243, 1121, 1450, 853, 882, 1180, 2458, 685, 1153, 1198, 837, 1622, 21, 2556, 284, 1637, 597, 913, 74, 1717, 1328, 1388, 1066, 869, 1281, 2023, 1901, 2171, 2709, 1651, 1534, 282, 950, 2254, 1189, 227, 1028, 2228, 2691, 207, 571, 243, 1291, 250, 2131, 1297, 1001, 2398, 1725, 1320, 51, 2243, 2909, 1117, 2528, 2639, 1048, 2814, 2525, 1406, 147, 525, 2835, 2050, 1920, 1866, 1603, 578, 1246, 678, 696, 1747, 2547, 1914, 2348, 129, 2185, 2271, 2011, 77, 406, 1253, 1930, 519, 1923, 487, 516, 438, 2021, 2881, 2813, 115, 2115, 377, 1722, 314, 1835, 1932, 2517, 1944, 2384, 425, 2665, 195, 770, 912, 2824, 371, 2523, 360, 2386, 2834, 2671, 1045, 1430, 634, 1054, 2655, 1490, 730, 1706, 2509, 2712, 420, 963, 1724, 1893, 253, 2826, 2700, 1184, 2516, 2089, 223, 2013, 815, 2335, 1458, 2543, 717, 936, 2276, 2438, 556, 1137, 2457, 1003, 2579, 72, 504, 2150, 1601, 2352, 2112, 1131, 1061, 672, 469, 2349, 1029, 1505, 1037, 2440, 1826, 1783, 204, 1611, 1353, 2652, 2259, 2563, 2382, 928, 2392, 1432, 1720, 1740, 2620, 756, 1838, 1111, 2014, 104, 1744, 300, 2713, 485, 126, 1115, 1304, 871, 1714, 2790, 2939, 1073, 2806, 922, 2182, 2462, 477, 178, 1146, 1536, 2444, 1359, 1239, 2748, 596, 281, 2121, 444, 2871, 1918, 2253, 673, 1479, 2330, 1064, 2205, 603, 926, 2346, 901, 2522, 1155, 1294, 275, 579, 149, 1702, 2026, 2710, 2082, 724, 2044, 2421, 1659, 1191, 591, 2406, 2040, 2491, 2908, 2565, 497, 507, 2858, 1442, 245, 764, 2436, 1909, 1950, 264, 2898, 2422, 1855, 661, 2235, 1616, 714, 1229, 2260, 431, 2740, 378, 45, 241, 1031, 1862, 1130, 2680, 130, 961, 2483, 1538, 242, 2287, 307, 2702, 2645, 2450, 850, 2644, 292, 1492, 1778, 2689, 1975, 796, 564, 2863, 2435, 229, 1846, 9, 1351, 2595, 2400, 914, 2718, 1295, 785, 858, 552, 495, 964, 838, 1648, 758, 341, 2564, 416, 2940, 57, 821, 1063, 684, 1483, 1096, 2918, 470, 1270, 2842, 2315, 939, 1333, 740, 2605, 1709, 1254, 1202, 1791, 312, 2372, 573, 172, 1645, 1928, 1262, 2258, 891, 1139, 1668, 1482, 78, 1423, 2305, 1264, 1728, 1417, 2764, 2199, 1770, 2051, 2542, 1790, 2241, 1692, 1336, 2443, 122, 2192, 2876, 109, 2076, 1455, 1708, 692, 875, 1761, 1162, 2728, 1341, 1355, 1192, 1272, 448, 2711, 2340, 1316, 1120, 1691, 710, 140, 620, 976, 2162, 1963, 2479, 1067, 1635, 404, 2106, 1581, 2742, 2582, 2126, 205, 197, 1412, 1119, 2576, 1750, 1175, 1662, 62, 2588, 520, 1697, 2343, 1805, 2359, 476, 2381, 2048, 1921, 2441, 849, 2365, 2874, 1109, 993, 2931, 2792, 2527, 1315, 10, 1009, 1900, 2091, 64, 474, 1116, 1703, 269, 2358, 989, 313, 1501, 523, 513, 503, 1015, 2489, 1044, 2147, 1511, 443, 2944, 630, 1160, 2086, 1933, 2859, 841, 270, 399, 2006, 844, 2558, 1569, 1642, 2531, 911, 1557, 2110, 2209, 177, 2309, 2747, 589, 2690, 2015, 2860, 1833, 809, 2640, 2116, 780, 2510, 707, 701, 1345, 677, 1300, 1247, 1451, 2714, 599, 166, 1019, 1002, 623, 273, 2264, 63, 1884, 2674, 582, 2280, 355, 464, 2893, 2096, 787, 2825, 459, 318, 1461, 2187, 659, 2661, 1647, 1357, 550, 979, 695, 1917, 286, 452, 2292, 779, 1643, 2321, 2538, 2865, 2660, 1514, 266, 2625, 1426, 1773, 287, 642, 2471, 2237, 70, 663, 1872, 2028, 1402, 1752, 840, 2914, 2016, 1983, 2296, 643, 2351, 2267, 1623, 493, 1845, 2290, 859, 447, 259, 1321, 2074, 1851, 2737, 1326, 2730, 283, 729, 652, 1194, 2560, 2363, 1760, 2550, 1673, 2353, 2631, 402, 1677, 1441, 1266, 968, 2827, 1815, 1330, 1924, 2153, 1960, 586, 1232, 1600, 2736, 37, 331, 2801, 1854, 2705, 2206, 2132, 1559, 2105, 2130, 998, 2897, 2492, 981, 301, 865, 2277, 2783, 739, 421, 2053, 2056, 2544, 2232, 2149, 1276, 1787, 1112, 2910, 434, 1650, 1132, 651, 119, 18, 1693, 1238, 1071, 110, 1493, 797, 342, 1152, 916, 1696, 86, 2600, 1955, 1032, 795, 1352, 1337, 1925, 2475, 108, 235, 889, 1954, 42, 2929, 2662, 502, 2496, 2886, 2613, 1005, 1285, 67, 2930, 1317, 2369, 1980, 592, 260, 1587, 316, 2779, 2059, 1959, 478, 289, 1282, 669, 219, 967, 1558, 31, 2540, 370, 1100, 1552, 1590, 2773, 2767, 2551, 2828, 1897, 1154, 2609, 2337, 2715, 726, 1010, 1560, 1730, 694, 2623, 590, 1999, 2139, 812, 25, 2084, 1527, 134, 2042, 1591, 2339, 788, 288, 2255, 364, 2781, 1566, 1459, 2920, 2902, 1000, 2719, 2839, 351, 2536, 1171, 1756, 1474, 2256, 458, 1024, 1344, 1405, 449, 1098, 2472, 1259, 144, 629, 2866, 2559, 2888, 927, 2158, 903, 1941, 1956, 1823, 635, 2467, 92, 54, 2098, 595, 2478, 297, 6, 1943, 2009, 1504, 1867, 874, 843, 2409, 1244, 2857, 414, 1305, 2079, 1213, 722, 492, 2697, 2188, 668, 2401, 1289, 706, 445, 2791, 1644, 248, 687, 1252, 1183, 453, 1572, 132, 1463, 1043, 2554, 1207, 36, 255, 1394, 133, 2428, 167, 1267, 909, 1195, 893, 338, 1976, 1503, 1561, 1322, 39, 2611, 1023, 2289, 2397, 851, 2137, 897, 319, 2646, 463, 2864, 2002, 2231, 26, 1375, 866, 2520, 2733, 1178, 2916, 2555, 8, 699, 2762, 2754, 899, 381, 1249, 466, 555, 1948, 2606, 193, 1803, 339, 2780, 917, 765, 576, 1077, 994, 53, 2882, 2354, 155, 330, 2774, 1284, 1327, 1830, 2503, 343, 128, 472, 2370, 2445, 1446, 335, 1595, 1798, 768, 1431, 2629, 11, 2926, 1588, 1540, 2937, 1670, 143, 468, 2302, 1716, 1818, 2642, 538, 1757, 2103, 1624, 97, 1288, 743, 1685, 2856, 1487, 1863, 854, 2686, 486, 123, 1334, 1663, 581, 1422, 390, 2399, 1981, 2797, 389, 1497, 1126, 1768, 91, 835, 2938, 1017, 2572, 857, 1093, 744, 1290, 1433, 1676, 1698, 1718, 291, 1556, 577, 422, 1711, 514, 2519, 1795, 1364, 2317, 1579, 2324, 2455, 47, 271, 2000, 1633, 142, 1298, 2080, 2190, 1695, 2624, 1870, 2500, 2819, 2766, 930, 1324, 1743, 1995, 2095, 2933, 1632, 2717, 214, 410, 920, 1549, 1946, 1968, 906, 957, 1800, 1735, 2104, 570, 433, 412, 2394, 2486, 2136, 585, 498, 2567, 956, 1182, 2848, 1040, 1583, 2032, 247, 2041, 192, 1822, 2208, 2682, 1230, 1400, 2164, 1883, 1517, 198, 2548, 1310, 1780, 1825, 2507, 1469, 781, 2138, 1880, 2537, 2843, 397, 1785, 824, 2884, 2685, 2389, 2218, 1512, 124, 2222, 1877, 600, 1258, 2404, 559, 2533, 89, 276, 734, 1371, 1613, 611, 2045, 368, 2316, 2257, 1065, 2087, 1775, 272, 1437, 1148, 340, 1081, 405, 2269, 1874, 95, 2037, 294, 1865, 1737, 136, 575, 374, 890, 1082, 2226, 1784, 2497, 2895, 2735, 1991, 633, 1265, 1894, 551, 819, 1649, 2119, 2703, 2085, 2118, 2672, 35, 1982, 1729, 567, 2181, 437, 2170, 2596, 880, 727, 2889, 2332, 2698, 2155, 2589, 2069, 2604, 1802, 2927, 593, 1260, 2756, 2592, 2753, 290, 2903, 1099, 176, 1421, 2299, 898, 2027, 1056, 417, 1016, 2418, 2838, 1532, 13, 862, 2380, 1847, 1387, 217, 467, 268, 1608, 988, 1447, 735, 2114, 1143, 1840, 2058, 742, 625, 1749, 1972, 2431, 202, 580, 2772, 2062, 1466, 2770, 1667, 2716, 1311, 2077, 2707, 1859, 1879, 2212, 1551, 1188, 2427, 457, 1687, 1, 2663, 2010, 1222, 1762, 509, 1014, 861, 1481, 1547, 813, 955, 1257, 2932, 257, 14, 2414, 2521, 649, 1030, 2285, 1806, 549, 328, 1997, 1533, 2319, 2360, 2549, 1726, 256, 2224, 2942, 2333, 2017, 1953, 1508, 980, 1567, 1380, 2402, 883, 2524, 2607, 1710, 2274, 1410, 2796, 73, 1361, 1248, 991, 1541, 475, 1393, 2219, 2793, 299, 1528, 2887, 521, 2539, 2511, 1170, 848, 982, 1832, 524, 363, 1526, 2514, 1435, 1684, 2498, 2367, 85, 1573, 2416, 215, 616, 352, 2323, 2667, 2060, 2599, 1812, 1473, 1754, 698, 2477, 1967, 2377, 56, 2094, 1261, 303, 1041, 3, 2532, 43, 2291, 152, 2207, 801, 958, 915, 2456, 1396, 2128, 1027, 1159, 2830, 877, 1235, 624, 2371, 2875, 855, 175, 1922, 609, 1489, 1500, 1764, 1217, 1837, 2025, 267, 1303, 2632, 2534, 587, 2403, 2347, 2425, 929, 2297, 1366, 1797, 2113, 17, 506, 1875, 2840, 1738, 2133, 610, 2574, 1660, 163, 997, 794, 718, 2043, 1607, 716, 415, 2088, 736, 2634, 1816, 704, 346, 2039, 2725, 563, 933, 626, 1403, 2018, 41, 210, 336, 2688, 1372, 1306, 1386, 1864, 403, 1988, 2075, 621, 1672, 1012, 344, 145, 1407, 1604, 2769, 1309, 2727, 2012, 823, 690, 2919, 885, 641, 2173, 2608, 879, 1767, 2849, 1937, 1485, 2676, 359, 937, 2923, 1342, 2890, 1453, 1367, 908, 170, 59, 1325, 2350, 2454, 618, 2341, 2750, 811, 619, 2635, 2107, 1236, 1070, 1753, 2413, 1915, 2239, 2334, 2437, 2807, 1440, 1006, 2378, 1484, 2684, 1781, 1614, 2488, 1543, 617, 1686, 1996, 2619, 383, 686, 1853, 1471, 1905, 1929, 1759, 2561, 2078, 1390, 1788, 1241, 2738, 1392, 1173, 456, 827, 1973, 1187, 2802, 2047, 1468, 1142, 1470, 1292, 2189, 2196, 2304, 2426, 1140, 1690, 691, 1548, 2614, 2230, 1665, 446, 2616, 1076, 873, 2482, 508, 440, 2313, 1664, 439, 1214, 1385, 262, 829, 1748, 2281, 500, 1628, 852, 2627, 20, 1074, 1986, 206, 786, 1675, 2159, 442, 2388, 277, 1011, 2657, 1449, 2001, 2820, 719, 125, 2729, 2266, 1167, 2868, 2279, 2451, 995, 2213, 407, 2782, 830, 943, 2423, 84, 1025, 959, 1250, 2038, 27, 2679, 2512, 413, 2312, 1888, 1652, 622, 2505, 76, 1869, 822, 2552, 886, 213, 2816, 1092, 353, 1308, 965, 501, 2626, 2836, 1899, 1819, 983, 1411, 748, 1240, 1627, 1499, 683, 302, 1958, 1113, 196, 693, 249, 2922, 2099, 1209, 1861, 2786, 2198, 992, 543, 50, 2393, 186, 666, 1669, 2268, 2282, 1654, 789, 327, 1771, 121, 137, 1221, 2284, 2203, 705, 833, 1088, 2726, 751, 1945, 2569, 2743, 391, 656, 1042, 2184, 2134, 2638, 1401, 2915, 1454, 2515, 2699, 473, 934, 1993, 423, 2545, 1961, 1586, 2708, 280, 2005, 2097, 187, 2395, 1898, 2197, 1974, 1985, 7, 2630, 800, 1079, 2100, 2648, 2913, 182, 738, 772, 2142, 2157, 419, 2405, 2776, 1158, 817, 614, 2617, 2852, 2573, 1984, 745, 1354, 1585, 2240, 1553, 2344, 349, 2160, 1273, 2928, 2854, 188, 565, 2144, 999, 816, 117, 90, 367, 2420, 2052, 1123, 1636, 1314, 2460, 2055, 2083, 1850, 2143, 2869, 1350, 2794, 776, 868, 505, 323, 2946, 1841, 1889, 2101, 1736, 1605, 2590, 2562, 1895, 1464, 2683, 2787, 2832, 969, 774, 2306, 2659, 96, 81, 1460, 2468, 1808, 975, 233, 2412, 1574, 2476, 2355, 488, 216, 1857, 553, 2681, 252, 325, 2362, 1114, 2186, 1007, 1203, 1546, 1656, 426, 2375, 2658, 762, 1399, 2183, 1369, 23, 396, 2907, 1966, 1104, 1419, 135, 1212, 1962, 2166, 904, 1824, 499, 2687, 2894, 2587, 892, 1903, 496, 1055, 1486, 810, 1052, 918, 1301, 2135, 345, 1849, 1518, 2861, 2746, 839, 432, 2732, 1168, 2120, 203, 2322, 1279, 1022, 1228, 1989, 802, 2870, 1977, 568, 2568, 375, 2034, 2721, 1742, 1278, 2046, 2601, 2615, 2809, 326, 2845, 1389, 1223, 1227, 534, 2911, 173, 1307, 2855, 2387, 1058, 876, 1568, 2583, 2651, 831, 2734, 845, 1161, 2167, 1804, 427, 2220, 211, 234, 1186, 99, 1166, 836, 401, 1521, 1765, 1828, 2178, 1231, 2246, 2163, 479, 1084, 2463, 531, 1097, 638, 386, 1519, 640, 2176, 1347, 1103, 2805, 2123, 1349, 2442, 169, 2518, 2408, 185, 263, 398, 200, 348, 921, 373, 1892, 2229, 1008, 1475, 931, 379, 116, 867, 480, 382, 1172, 87, 1331, 2283, 1529, 1965, 971, 533, 511, 28, 972, 465, 697, 2288, 2204, 2172, 2434, 1118, 2900, 1200, 1286, 1196, 2844, 1772, 2217, 895, 1913, 2262, 265, 1786, 1911, 798, 572, 1245, 662, 606, 2248, 583, 44, 1283, 454, 1836, 161, 2878, 2067, 2072, 1150, 100, 1507, 1362, 602, 2621, 113, 974, 1666, 594, 2693, 2194, 1734, 522, 483, 948, 1705, 1515, 30, 792, 1413, 2877, 2912, 1554, 1634, 2464, 775, 608, 1201, 1416, 962, 2921, 2901, 1234, 1429, 2356, 708, 1807, 418, 2529, 392, 777, 2701, 517, 1107, 1477, 450, 1368, 1653, 111, 1712, 2546, 1831, 1610, 1313, 860, 2757, 1814, 1544, 2586, 46, 2785, 2331, 1732, 1287, 183, 1689, 164, 1763, 2008, 2501, 1902, 2193, 2244, 1269, 1280, 1912, 148, 1926, 2368, 761, 667, 1755, 118, 536, 518, 337, 1225, 537, 1661, 2905, 2109, 2432, 675, 703, 888, 482, 2815, 191, 1080, 612, 605, 2124, 171, 1700, 2238, 1852, 1124, 251, 2318, 114, 932, 428, 657, 1033, 973, 2513, 2156, 1658, 2111, 530, 38, 88, 1185, 2741, 2755, 1094, 2760, 2706, 1916, 2168, 1319, 222, 1296, 2214, 1085, 2092, 2247, 127, 1680, 752, 2357, 2033, 1881, 393, 547, 966, 68, 131, 306, 1885, 1639, 2650, 2385, 818, 33, 83, 1138, 2419, 2485, 2029, 681, 1164, 429, 554, 615, 174, 2169, 2151, 1452, 1363, 671, 2461, 2191, 2448, 1378, 1570, 60, 2850, 2695, 1910, 408, 1640, 1381, 2610, 545, 2030, 985, 1609, 2325, 2731, 1418, 1858, 94, 702, 1395, 2364, 1630, 1936, 1199, 19, 655, 101, 2295, 1539, 1089, 395, 484, 1777, 2446, 1964, 1204, 887, 757, 1051, 1377, 2275, 1211, 2696, 2758, 1619, 2841, 2947, 978, 1766, 2366, 2800, 2242, 12, 601, 631, 69, 65, 1550, 49, 1834, 828, 535, 304, 1193, 1329, 2749, 366, 1908, 2810, 153, 1102, 986, 1323, 2216, 1842, 2904, 2081, 881, 2692, 1951, 1480, 2108, 526, 424, 540, 1935, 2829, 2481, 713, 2817, 925, 1465, 347, 1934, 733, 2495, 2789, 1053, 2603, 1523, 1087, 1694, 1472, 179, 436, 1701, 2004, 2177, 1699, 680, 295, 628, 1415, 2338, 791, 2410, 246, 2447, 93, 1820, 2823, 1205, 1638, 2570, 2093, 1144, 1597, 329, 154, 2211, 1576, 372, 1535, 732, 2553, 2433, 2761, 1424, 2251, 2831, 1050, 1163, 15, 2670, 941, 221, 1373, 240, 1219, 1970, 896, 2833, 2311, 1563, 2179, 1598, 2221, 1275, 2019, 512, 1564, 2846, 660, 1034, 650, 1091, 1776, 2594, 350, 763, 1409, 529, 189, 639, 753, 310, 80, 138, 141, 29, 1136, 460, 1873, 1931, 548, 274, 1731, 947, 825, 938, 1467, 24, 588, 2407, 1122, 2673, 644, 1448, 4, 2677, 2771, 528, 804, 1774, 1927, 790, 52, 1510, 1456, 22, 1620, 369, 1829, 1039, 1886, 637, 1129, 1617, 2675, 1127, 1348, 2487, 1383, 1498, 435, 910, 394, 1562, 315, 769, 1674, 208, 2154, 491, 1338, 1707, 2494, 1782, 2036, 2668, 2526, 209, 700, 2654, 120, 803, 1979, 1947, 2571, 1878, 1108, 1398, 2125, 557, 254, 1721, 1719, 1688, 2873, 2383, 541, 2637, 1339, 2480, 755, 1789, 951, 1555, 766, 1907, 2474, 1890, 278, 2612, 1542, 190, 2215, 2822, 1462, 907, 385, 1745, 1641, 2223, 400, 1606, 1799, 2396, 2906, 1919, 527, 2862, 1427, 863, 782, 2812, 2070, 225, 2925, 2391, 2759, 1049, 2891, 905, 653, 320, 2066, 1176, 2390, 953, 156, 2175, 2090, 2557, 1589, 1277, 749, 34, 494, 244, 471, 1494, 180, 2575, 771, 2597, 1531, 309, 1335, 2924, 239, 1414, 2628, 2180, 32, 1360, 411, 184, 2936, 2294, 1758, 380, 754, 613, 1612, 2768, 1811, 2141, 1779, 832, 2233, 747, 1274, 238, 2140, 162, 793, 2598, 2236, 1839, 2148, 1101, 1312, 2643, 2837, 1813, 1090, 1957, 1615, 105, 430, 106, 2314, 1631, 1457, 715, 970, 2724, 737, 1843, 2278, 1436, 574, 332, 665, 71, 1197, 806, 2303, 977, 2577, 2584, 2064, 2723, 2535, 324, 2245, 2647, 1733, 2465, 2336, 461, 285, 311, 984, 645, 1149, 2373, 1391, 160, 1106, 1671, 1593, 679, 746, 2530, 799, 2342, 321, 258, 1952, 2449, 2821, 455, 712, 902, 66, 1095, 566, 2469, 1078, 1376, 279, 451, 1069, 361, 2896, 220, 2210, 1998, 1971, 2934, 778, 237, 356, 2024, 1060, 194, 2943, 2, 362, 298, 2666, 569, 1218, 2541, 510, 1844, 376, 333, 2049, 2301, 682, 1887, 1906, 842, 1133, 1013, 2063, 1179, 1618, 58, 2618, 1496, 2003, 894, 1524, 1346, 1723, 1491, 632, 201, 2374, 2484, 357, 2453, 1876, 1848, 1374, 1949, 2765, 846, 1004, 539, 5, 872, 1592, 2851, 1206, 2704, 1420, 2506, 604, 2664, 1072, 261, 2272, 1210, 2065, 1075, 1086, 1809, 1530, 1513, 334, 0]\n"
     ]
    }
   ],
   "source": [
    "print(nn[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_R_precission = {}\n",
    "for (att_idx, att) in [(0, \"nom\"), (1, \"cognom_1\"), (2, \"cognom_2\")]:\n",
    "    metrics_R_precission[att] = {}\n",
    "    att_mean_metric = 0\n",
    "    order_ocrs_individuals = np.array(list_ocrs[att])\n",
    "    for ind_idx in range(language_embeddings.shape[0]):\n",
    "        \n",
    "        try:\n",
    "            content_attribute = graph[att].map_index_attribute[ind_idx]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        relevant_individuals = graph[att].map_attribute_index[content_attribute]\n",
    "        relevant_languages = order_ocrs_individuals[relevant_individuals]\n",
    "               \n",
    "        #if ind_idx == 2948:\n",
    "        #    print(relevant_languages)\n",
    "        #    break\n",
    "        \n",
    "        value = r_precision(relevant_documents=relevant_languages, retrieved_documents=nn[0][ind_idx])\n",
    "        att_mean_metric += value\n",
    "        if metrics_R_precission.get(content_attribute, None) is None:\n",
    "            metrics_R_precission[att][content_attribute] = [value * 100]\n",
    "            \n",
    "        else:\n",
    "            metrics_R_precission[att][content_attribute].append(value)\n",
    "            \n",
    "    metrics_R_precission[att][\"mean_recall\"] = att_mean_metric/len(metrics_R_precission[att])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005385880091916884"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_R_precission[\"cognom_2\"][\"mean_recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.010 nom\n",
    "#0.04 cognom\n",
    "#0.03 cognom_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric space of the visual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn, distances = extract_intra_cluster_nearest_neighboors_at_k(attribute_embeddings, top_k=attribute_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13829, 3, 128])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jaume'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[\"nom\"].map_index_attribute[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_freq  = sorted(list(computed_freq_name.items()), key= lambda k: (k[1]), reverse=True)\n",
    "surname_freq  = sorted(list(computed_freq_surname.items()), key= lambda k: (k[1]), reverse=True)\n",
    "second_surnname_freq  = sorted(list(computed_freq_ssurname.items()), key= lambda k: (k[1]), reverse=True)\n",
    "\n",
    "name_high_freq, name_low_freq = [i[0] for i in name_freq if i[1] >= 100],  [i[0] for i in name_freq if i[1] <= 20] \n",
    "surname_high_freq, surname_low_freq = [i[0] for i in surname_freq if i[1] >= 100],  [i[0] for i in surname_freq if i[1] <= 20] \n",
    "ssurname_high_freq, ssurname_low_freq = [i[0] for i in second_surnname_freq if i[1] >= 100],  [i[0] for i in second_surnname_freq if i[1] <= 20] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_R_precission = {}\n",
    "for (att_idx, att, high_freq, low_freq) in [(0, \"nom\", name_high_freq, name_low_freq), (1, \"cognom_1\", surname_high_freq, surname_low_freq), (2, \"cognom_2\", ssurname_high_freq, ssurname_low_freq)]:\n",
    "    metrics_R_precission[att] = {}\n",
    "    att_mean_low = 0\n",
    "    att_mean_high = 0\n",
    "    att_mean = 0\n",
    "    count_high = 0\n",
    "    count_low = 0\n",
    "    count = 0\n",
    "    for ind_idx in range(attribute_embeddings.shape[0]):\n",
    "        \n",
    "        try:\n",
    "            content_attribute = graph[att].map_index_attribute[ind_idx]\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if metrics_R_precission.get(content_attribute, None) is None:\n",
    "            metrics_R_precission[att][content_attribute] = []\n",
    "        \n",
    "        \n",
    "        relevant_individuals = graph[att].map_attribute_index[content_attribute]\n",
    "        value = r_precision(relevant_documents=relevant_individuals, retrieved_documents=nn[att_idx][ind_idx])\n",
    "        att_mean += value\n",
    "        \n",
    "        if content_attribute in high_freq:\n",
    "            count_high += 1\n",
    "            att_mean_high += value           \n",
    "        \n",
    "        elif content_attribute in low_freq:\n",
    "            count_low += 1\n",
    "            att_mean_low += value           \n",
    "\n",
    "        metrics_R_precission[att][content_attribute].append(value)\n",
    "                \n",
    "    metrics_R_precission[att][\"high_mean_recall\"] = att_mean_high/(count_high)\n",
    "    metrics_R_precission[att][\"low_mean_recall\"] = att_mean_low/(count_low)\n",
    "    metrics_R_precission[att][\"mean_recall\"] = att_mean/(attribute_embeddings.shape[0])\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02878049812218453\n",
      "0.041112067630115136\n",
      "0.0008279450032342326\n"
     ]
    }
   ],
   "source": [
    "print(metrics_R_precission[\"nom\"][\"mean_recall\"])\n",
    "print(metrics_R_precission[\"nom\"][\"high_mean_recall\"])\n",
    "print(metrics_R_precission[\"nom\"][\"low_mean_recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016342247519383804\n",
      "0.018197421607707962\n",
      "0.02151953542645649\n"
     ]
    }
   ],
   "source": [
    "print(metrics_R_precission[\"cognom_1\"][\"mean_recall\"])\n",
    "print(metrics_R_precission[\"cognom_1\"][\"high_mean_recall\"])\n",
    "print(metrics_R_precission[\"cognom_1\"][\"low_mean_recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014176128352925053\n",
      "0.01633218919423515\n",
      "0.016796353925177802\n"
     ]
    }
   ],
   "source": [
    "print(metrics_R_precission[\"cognom_2\"][\"mean_recall\"])\n",
    "print(metrics_R_precission[\"cognom_2\"][\"high_mean_recall\"])\n",
    "print(metrics_R_precission[\"cognom_2\"][\"low_mean_recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Linkage Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the inter attribute metric space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n",
      "Fold 1:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n",
      "Fold 2:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n",
      "Fold 3:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n",
      "Fold 4:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n",
      "Fold 5:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n",
      "Fold 6:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n",
      "Fold 7:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n",
      "Fold 8:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n",
      "Fold 9:\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 1\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 3\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 5\n",
      "\n",
      "STARTING THE EVALUATION WITH KNN, n_neighbors = 10\n"
     ]
    }
   ],
   "source": [
    "knn_metr = utils.evaluate_attribute_metric_space(attribute_embeddings.numpy(), plot_path=os.path.join(base_save_plot, f\"inter_attr_distribution.jpg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inter_att_dist': {'Accuracy_dist': {1: (1.0, 0.0),\n",
       "   3: (1.0, 0.0),\n",
       "   5: (1.0, 0.0),\n",
       "   10: (1.0, 0.0)},\n",
       "  'F_score_dist': {1: (1.0, 0.0),\n",
       "   3: (1.0, 0.0),\n",
       "   5: (1.0, 0.0),\n",
       "   10: (1.0, 0.0)}}}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_metr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Linkage Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTING LANGUAGE EMBEDDINGS\n",
      "EXTRACTING VISUAL EMBEDDINGS\n",
      "Embeddings Loaded Succesfully\n"
     ]
    }
   ],
   "source": [
    "checkpoint_name = \"MMGC_Experiment_1_New_Edge_PE_no_Attention_language/MMGC_Experiment_1_New_Edge_PE_no_Attention_language_train_199\"\n",
    "print(\"EXTRACTING LANGUAGE EMBEDDINGS\")\n",
    "filepath  = f\"../embeddings/language_embeddings_{number_volum_years}_entities_{len(cfg_setup.configuration.compute_loss_on)}.pkl\"\n",
    "language_embeddings = utils.read_pickle(filepath)\n",
    "\n",
    "print(\"EXTRACTING VISUAL EMBEDDINGS\")\n",
    "attribute_embeddings = utils.read_pickle(filepath=f\"../embeddings/{checkpoint_name}.pkl\")\n",
    "attribute_embeddings = attribute_embeddings.cpu()\n",
    "\n",
    "\n",
    "print(\"Embeddings Loaded Succesfully\")\n",
    "\n",
    "graph = graphset._graph\n",
    "graph.x_language = language_embeddings\n",
    "graph.epoch_populations = image_dataset._population_per_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pairs = graph[(\"individual\", \"similar\", \"individual\")].negative_sampling\n",
    "true_pairs = graph[(\"individual\", \"similar\", \"individual\")].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the population from the last time\n",
    "final_time_gap_population = list(graph.epoch_populations[-2]) + list(graph.epoch_populations[-1]) \n",
    "final_time_gap_population = torch.tensor(final_time_gap_population).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.epoch_populations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking the population that appears in the last pair of periods (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.isin(true_pairs[:2, :], final_time_gap_population).all(dim=0)\n",
    "mask_candidate = torch.isin(candidate_pairs[:2,:], final_time_gap_population).all(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_true_pairs_subgraph = true_pairs[:, mask]\n",
    "specific_subgraph_candidate = candidate_pairs[:, mask_candidate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test same as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_indexes = torch.cat((specific_true_pairs_subgraph, specific_subgraph_candidate), dim=1).type(torch.int32).numpy()\n",
    "X_test = (attribute_embeddings[X_test_indexes[0], :] - attribute_embeddings[X_test_indexes[1],:]).pow(2).sum(-1).sqrt()\n",
    "y_test = X_test_indexes[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ^ TRAIN EXTRACTION \n",
    "earlies_time_populations = []\n",
    "for pop in graph.epoch_populations[:-1]: ## Keep the las two periods\n",
    "    earlies_time_populations += list(pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrat the population of the first periods to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlies_time_populations = torch.tensor(earlies_time_populations).type(torch.int64)\n",
    "mask = torch.isin(true_pairs[:2, :], earlies_time_populations).all(dim=0)\n",
    "mask_candidate_train = torch.isin(candidate_pairs[:2,:], earlies_time_populations).all(dim=0)\n",
    "\n",
    "specific_true_pairs_subgraph_train = true_pairs[:, mask]\n",
    "specific_subgraph_candidate_train = candidate_pairs[:, mask_candidate_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indexes = torch.cat((specific_true_pairs_subgraph_train, specific_subgraph_candidate_train), dim=1).numpy()\n",
    "X_train = (attribute_embeddings[X_train_indexes[0], :] - attribute_embeddings[X_train_indexes[1],:]).pow(2).sum(-1).sqrt()\n",
    "\n",
    "y_train = X_train_indexes[-1] #torch.cat((torch.ones((specific_true_pairs_subgraph_train.shape[1])), torch.zeros((specific_subgraph_candidate_train.shape[1]))), dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3911.0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN Probability:  0.5\n",
      "STD Probability:  0.08280965206569965\n",
      "Confusion Matrix:\n",
      " [[ 154 4484]\n",
      " [ 294 2131]]\n",
      "Accuracy: 0.3235169191561659\n",
      "Recall:  0.8787628865979381\n",
      "Precission:  0.3221466364323507\n",
      "F-score:  0.47146017699115034\n",
      "Specificity:  0.044746787603930464\n"
     ]
    }
   ],
   "source": [
    "metrics_rl = rl.record_linkage_with_logistic_regression(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, candidate_sa=X_test_indexes,\n",
    "                                                                                                                random_state=0, penalty=\"l2\", class_weight=\"balanced\", n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

verbose: True
log_wandb: True


data:


  datasets:

    IAM:
      _target_: pretrain.IAMDataset
      path: "/data/users/cboned/data/HTR/IAM"  

    Esposalles:
      _target_: pretrain.EsposallesDataset
      path: "/data/users/cboned/data/HTR/Esposalles" 

    Washington:
      _target_: pretrain.WashingtonDataset
      path: "/data/users/cboned/data/HTR/Washington" 

  collator:
    shuffle: True
    batch_size: 32
    partitions_ratio: [0.9, 0.05, 0.05]



models:


  finetune: False
  add_visual_encoder: True
  add_language: False
  name_checkpoint: "Autoencoder Pretained on IAM + Esposalles + Washington"


  visual_encoder:
    input_channels: 3
    hidden_channels: 32
    output_channels: 300
    number_of_hidden_convolutions: 3
    kernel_height: [5, 5, 5, 5, 5]
    kernel_width: [3, 3, 3, 3, 3]


setup:



  configuration:
    batch_size: ${data.collator.batch_size}
    epochs: 200
    embedding_size: ${models.visual_encoder.output_channels}
    lr: 1e-4
    visual_encoder_convolutions: ${models.visual_encoder.number_of_hidden_convolutions}
    visual_encoder_hidden_channels: ${models.visual_encoder.hidden_channels}

  optimizer:
    _target_: torch.optim.Adam
    lr: ${setup.configuration.lr}

  
  wandb:
    project: "Graph Construction"
    config: ${setup.configuration}
    name: "Pretraining on IAM + Washington + Esposalles"
    group: "Autoencoder Pretraining"





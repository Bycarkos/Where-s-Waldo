verbose: True
log_wandb: True


data:

  import_data: True
  export_data: True

  dataset:
    path: "/data/users/cboned/data/HTR/CED/SFLL"
    volumes_years: [1889, 1906]
    attributes: ["nom", "cognom_1", "cognom_2"]
    patch_size: 16
    cut_image: True


  graph_configuration:
    attribute_type_of_nodes: ["nom", "cognom_1", "cognom_2"]
    entity_type_of_nodes: ["individual", "image"]
    attribute_edges: ["nom", "cognom_1", "cognom_2"]
    entity_edges : ["same_as", "family"]
    node_embedding_size: ${models.gnn_encoder.model.embedding_size}
    edge_embedding_size: ${models.gnn_encoder.model.embedding_size}

  collator:
    shuffle: True
    batch_size: 128
    partitions_ratio: [0.9, 0.05, 0.05]



models:


  add_language: False
  name_checkpoint: "checkpoints/GraphConstruction/Experiment1.pt"

  visual_encoder:

    model:
      _target_: train.LineAutoEncoder
      input_channels: 3
      hidden_channels: 32
      output_channels: 300
      num_middle_conv: 3

    finetune: True
    freeze: True
    checkpoint: "checkpoints/AutoEncoders/Autoencoder Pretrained on Esposalles.pt"


  edge_visual_encoder:
    
    model:
      _target_: train.DisentanglementAttentionEncoder
      in_features: 300
      out_features: 300

    finetune: False
    freeze: False
    checkpoint: " "


  gnn_encoder:

    model:
      _target_: train.AttributeGNN
      embedding_size: ${models.edge_visual_encoder.model.out_features}

    finetune: False
    freeze: True
    checkpoint: " "

setup:

  configuration:
    optimize_task: Loss
    batch_size: ${data.collator.batch_size}
    patch_size: 16
    epochs: 200
    dataset: CED-SFLL
    lr: 1e-4
    compute_loss_on: ["nom", "cognom_1", "cognom_2", "individual"]


  optimizer:
    _target_: torch.optim.Adam
    lr: ${setup.configuration.lr}

  
  wandb:
    project: "Graph Construction"
    config: ${setup.configuration}
    name: "First Experiment Setup to check that all is fine"
    group: "Graph Construction"




